{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deng-Xian-Sheng/Real-technology/blob/main/%E2%80%9CDeepSeek_R1_Distill_Qwen_1_5B_Conversational_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "要在免费的 Tesla T4 Google Colab 实例上运行此程序，请点击“运行时”，然后点击“全部运行”。\n",
        "\n",
        "   如果您需要帮助，请加入 Discord + ⭐ 在 Github 上给我们点赞 ⭐\n",
        "要在您自己的电脑上安装 Unsloth，请按照我们 Github 页面上的安装说明进行操作 [redacted link]。\n",
        "\n",
        "您将学习如何进行[redacted link]、如何[redacted link]、如何[redacted link] 以及[redacted link]。\n",
        "\n",
        "访问我们的文档以获取我们所有的[redacted link] 和[redacted link]。"
      ],
      "metadata": {
        "id": "-Xf3rgc7QUAa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYKljk5LIiLE"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[新增] 我们修复了 Phi-4 中的许多错误，这大大提高了 Phi-4 的准确性。请参阅我们的[redacted link]。\n",
        "\n",
        "[新增]** 您可以查看所有包含我们错误修复的 Phi-4 模型上传，包括[redacted link]、GGUF 等[redacted link]。\n",
        "\n",
        "[新增] 截至 2024 年 11 月，Unsloth 现在支持[redacted link]！\n",
        "\n",
        "希望这可以帮到您！如果您有任何其他问题，请告诉我。"
      ],
      "metadata": {
        "id": "xx7h-xnhPVkk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahMyuVuRIiLG"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "source": [
        "%%capture\n",
        "# 通常情况下，使用 pip install unsloth 就足够了\n",
        "\n",
        "# 截至 2025 年 1 月 31 日，Colab 暂时存在一些与 Pytorch 相关的问题\n",
        "# 使用 pip install unsloth 将花费 3 分钟，而以下方法花费不到 1 分钟：\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Bam5RdZxJVF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMa-WvWfIiLH"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "source": [
        "from unsloth import FastLanguageModel  # 从 unsloth 库导入 FastLanguageModel 类\n",
        "import torch  # 导入 PyTorch 库\n",
        "\n",
        "max_seq_length = 2048  # 设置最大序列长度，此处为 2048，支持 RoPE Scaling 自动缩放\n",
        "dtype = None  # 数据类型，设置为 None 自动检测，Tesla T4、V100 使用 Float16，Ampere+ 使用 Bfloat16\n",
        "load_in_4bit = True  # 使用 4 位量化减少内存使用，可以设置为 False\n",
        "\n",
        "# 支持的 4 位预量化模型，下载速度提升 4 倍，避免内存不足错误\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",  # Llama-3.1，速度提升 2 倍\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Mistral-Small-Instruct-2409\",  # Mistral 22b，速度提升 2 倍！\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",  # Phi-3.5，速度提升 2 倍！\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",  # Gemma，速度提升 2 倍！\n",
        "    \"unsloth/Llama-3.2-1B-bnb-4bit\",  # 全新！Llama 3.2 模型\n",
        "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "]  # 更多模型请访问 https://huggingface.co/unsloth\n",
        "\n",
        "qwen_models = [\n",
        "    \"unsloth/Qwen2.5-Coder-32B-Instruct\",  # Qwen 2.5 Coder，速度提升 2 倍\n",
        "    \"unsloth/Qwen2.5-Coder-7B\",\n",
        "    \"unsloth/Qwen2.5-14B-Instruct\",  # 14B 参数模型适用于 16GB 显卡\n",
        "    \"unsloth/Qwen2.5-7B\",\n",
        "    \"unsloth/Qwen2.5-72B-Instruct\",  # 72B 参数模型适用于 48GB 显卡\n",
        "    \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit\"\n",
        "]  # 更多模型请访问 https://huggingface.co/unsloth\n",
        "\n",
        "# 从预训练模型加载模型和分词器\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit\",  # 模型名称\n",
        "    max_seq_length=max_seq_length,  # 最大序列长度\n",
        "    dtype=dtype,  # 数据类型\n",
        "    load_in_4bit=load_in_4bit,  # 是否使用 4 位量化加载\n",
        "    # token=\"hf_...\",  # 如果使用 gated 模型（如 meta-llama/Llama-2-7b-hf），请使用此参数\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vlp2cgfwJgby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们现在添加 LoRA 适配器，这样我们只需要更新 1% 到 10% 的所有参数！"
      ],
      "metadata": {
        "id": "aOFjGslGT6L_"
      }
    },
    {
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,  # 选择任意大于 0 的数字！建议值：8、16、32、64、128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],  # 应用 LoRA 的目标模块\n",
        "    lora_alpha = 16,  # LoRA 的缩放因子\n",
        "    lora_dropout = 0,  # 支持任意值，但 0 是最优化的\n",
        "    bias = \"none\",  # 支持任意值，但 \"none\" 是最优化的\n",
        "    # [新功能] \"unsloth\" 减少 30% 的显存使用，适用于 2 倍大的批次大小！\n",
        "    use_gradient_checkpointing = \"unsloth\",  # 对于非常长的上下文，设置为 True 或 \"unsloth\"\n",
        "    random_state = 3407,  # 随机种子\n",
        "    use_rslora = False,  # 支持秩稳定 LoRA\n",
        "    loftq_config = None,  # 支持 LoftQ\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gILnkTdxJot6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 数据准备\n",
        "现在，我们使用 Qwen-2.5 格式进行对话风格的微调。我们使用 mlabonne/FineTome-100k 数据集，该数据集采用 ShareGPT 风格。但是，我们将其转换为 HuggingFace 的标准多轮对话格式 (\"role\", \"content\")，而不是 (\"from\", \"value\")。Qwen 渲染多轮对话的方式如下：\n",
        "\n",
        "```\n",
        "<|im_start|>system\n",
        "你是文心一言，由阿里云创造。你是一个乐于助人的助手。<|im_end|>\n",
        "<|im_start|>user\n",
        "2+2 等于多少？<|im_end|>\n",
        "<|im_start|>assistant\n",
        "等于 4。<|im_end|>\n",
        "```\n",
        "我们使用 get_chat_template 函数来获取正确的对话模板。我们支持 zephyr、chatml、mistral、llama、alpaca、vicuna、vicuna_old、phi3、llama3 等多种对话模板。"
      ],
      "metadata": {
        "id": "xPlmVjHFKFN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"qwen-2.5\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "HOYfW6RIRNNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }"
      ],
      "metadata": {
        "id": "wgQM3H9dRcsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# 挂载谷歌drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "qEeDU16j36gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "######\n",
        "#从json加载数据集\n",
        "######\n",
        "\n",
        "如果你的数据集是：\n",
        "[\n",
        "  [\n",
        "    {\n",
        "      \"role\":\"\",\n",
        "      \"content\":\"\"\n",
        "    }\n",
        "  ]\n",
        "]\n",
        "\n",
        "则需要：\n",
        "import json\n",
        "from datasets import Dataset\n",
        "# 手动加载 JSON 文件，将每个对话列表包装到 \"conversations\" 字段\n",
        "with open('./drive/MyDrive/Colab Notebooks/xiaoshuo_dataset.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "  # 将每个对话列表包装为 {\"conversations\": [...]}\n",
        "  wrapped_data = [{\"conversations\": convo} for convo in data]\n",
        "  dataset = Dataset.from_list(wrapped_data)\n",
        "\n",
        "请尽可能的预先把数据集做成：\n",
        "[\n",
        "  {\n",
        "    \"conversations\": [\n",
        "      {\n",
        "        \"role\":\"\",\n",
        "        \"content\":\"<think>嗯，这个问题……</think>你需要……\"\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "######\n",
        "#从HF加载数据集\n",
        "######\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")\n",
        "\"\"\"\n",
        "\n",
        "# 从json加载数据集但符合格式\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\",data_files='./drive/MyDrive/Colab Notebooks/xiaoshuo_dataset.json', split = \"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们现在使用 standardize_sharegpt 将 ShareGPT 风格的数据集转换为 HuggingFace 的通用格式。这会将数据集从以下形式：\n",
        "\n",
        "```\n",
        "{\"from\": \"system\", \"value\": \"You are an assistant\"}\n",
        "{\"from\": \"human\", \"value\": \"What is 2+2?\"}\n",
        "{\"from\": \"gpt\", \"value\": \"It's 4.\"}\n",
        "```\n",
        "转换为以下形式：\n",
        "```\n",
        "{\"role\": \"system\", \"content\": \"You are an assistant\"}\n",
        "{\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
        "{\"role\": \"assistant\", \"content\": \"It's 4.\"}\n",
        "```\n"
      ],
      "metadata": {
        "id": "im4a0__EKWhw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPXzJZzHEgXe"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "# dataset = standardize_sharegpt(dataset) # 禁用此行，除非你需要转换\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们来看看项目5的对话结构："
      ],
      "metadata": {
        "id": "6Qa0x7m3qK_D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGFzmplrEy9I"
      },
      "outputs": [],
      "source": [
        "dataset[5][\"conversations\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "然后我们看看聊天模板是如何转换这些对话的。"
      ],
      "metadata": {
        "id": "0SJSyiOUqjpB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhXv0xFMGNKE"
      },
      "outputs": [],
      "source": [
        "dataset[5][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 模型训练\n",
        "现在，让我们使用 Huggingface TRL 库的 SFTTrainer！更多文档请参考：[redacted link]。\n",
        "\n",
        "为了加快训练速度，我们只进行 60 步迭代，但您可以设置 num_train_epochs=1 进行完整训练，并注释max_steps或设为0。\n",
        "\n",
        "我们也支持 TRL 的 DPOTrainer！\n",
        "\n",
        "此训练器包含我们对 梯度累积错误 的修复。想了解更多信息，请阅读：[redacted link]"
      ],
      "metadata": {
        "id": "2QFxSuxOKq12"
      }
    },
    {
      "source": [
        "from trl import SFTTrainer  # 从 trl 库导入 SFTTrainer 类，用于监督式微调\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq  # 从 transformers 库导入训练参数和数据整理器\n",
        "from unsloth import is_bfloat16_supported  # 从 unsloth 库导入 is_bfloat16_supported 函数，用于检查是否支持 bf16 数据类型\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(  # 创建一个 SFTTrainer 实例\n",
        "    model=model,  # 要微调的模型\n",
        "    tokenizer=tokenizer,  # 分词器\n",
        "    train_dataset=dataset,  # 训练数据集\n",
        "    dataset_text_field=\"text\",  # 数据集中文本字段的名称\n",
        "    max_seq_length=max_seq_length,  # 最大序列长度\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),  # 数据整理器，用于将数据整理成模型所需的格式\n",
        "    dataset_num_proc=4,  # 用于数据处理的进程数\n",
        "    packing=False,  # 是否打包数据，对于短序列可以加快训练速度 5 倍\n",
        "    args=TrainingArguments(  # 训练参数\n",
        "        per_device_train_batch_size=1,  # 每个设备的训练批次大小\n",
        "        # per_device_train_batch_size=4,  # 每个设备的训练批次大小\n",
        "        gradient_accumulation_steps=4,  # 梯度累积步数，最新版 Unsloth 修复了一个主要错误\n",
        "        # num_train_epochs=1,  # 训练轮数，设置为 1 进行完整训练\n",
        "        warmup_steps=5,  # 预热步数\n",
        "        max_steps=30,  # 最大训练步数\n",
        "        # warmup_steps=666,  # 预热步数\n",
        "        # max_steps=4000,  # 最大训练步数\n",
        "        learning_rate=2e-4,  # 学习率\n",
        "        fp16=not is_bfloat16_supported(),  # 是否使用 fp16 数据类型\n",
        "        bf16=is_bfloat16_supported(),  # 是否使用 bf16 数据类型\n",
        "        logging_steps=1,  # 日志记录步数\n",
        "        optim=\"paged_adamw_8bit\",  # 优化器，使用 paged_adamw_8bit 节省更多内存\n",
        "        weight_decay=0.01,  # 权重衰减\n",
        "        lr_scheduler_type=\"linear\",  # 学习率调度器类型\n",
        "        seed=3407,  # 随机种子\n",
        "        output_dir=\"outputs\",  # 输出目录\n",
        "        report_to=\"none\",  # 是否将训练报告发送到其他平台，例如 WandB\n",
        "        # report_to=\"WandB\",  # 是否将训练报告发送到其他平台，例如 WandB\n",
        "    ),\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "pfhxIO7dK31H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们还使用 Unsloth 的 train_on_completions 方法，仅对助手的输出进行训练，而忽略用户输入的损失。"
      ],
      "metadata": {
        "id": "REJYb5CDK-Jn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juQiExuBG5Bt"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|im_start|>user\\n\",\n",
        "    response_part = \"<|im_start|>assistant\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们验证掩码是否已实际应用："
      ],
      "metadata": {
        "id": "fo3R6jfnLFDt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtsMVtlkUhja"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rD6fl8EUxnG"
      },
      "outputs": [],
      "source": [
        "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
        "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以看到系统和指令提示已被成功屏蔽！"
      ],
      "metadata": {
        "id": "u-rstpFJLN6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 显示当前内存统计信息\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "uToiZUlrLmys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们修复了所有训练器中的一个主要的梯度累积错误。有关更多详细信息，请参阅[redacted link]。"
      ],
      "metadata": {
        "id": "dTnprb-_LvAW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "# 开始训练\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title 显示最终内存和时间统计信息\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 推理\n",
        "让我们运行模型！您可以更改指令和输入 - 将输出留空！\n",
        "\n",
        "[新功能] 在免费的 Colab 中尝试使用 Llama-3.1 8b Instruct 进行 2 倍速推理 [此处](已删除链接)\n",
        "\n",
        "我们使用 min_p = 0.1 和 temperature = 1.5。阅读此 [推文] (已删除链接)以了解更多信息。"
      ],
      "metadata": {
        "id": "udjArXaXMGj7"
      }
    },
    {
      "source": [
        "from unsloth.chat_templates import get_chat_template  # 从 unsloth 库导入 get_chat_template 函数\n",
        "\n",
        "# 获取 Qwen-2.5 的聊天模板，并应用到分词器\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"qwen-2.5\",\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model)  # 启用原生 2 倍速推理\n",
        "\n",
        "# 定义消息列表，包含用户角色和内容\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},  # 用户要求继续斐波那契数列\n",
        "]\n",
        "\n",
        "# 使用聊天模板处理消息，进行分词，添加生成提示，并转换为 PyTorch 张量\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,  # 必须添加用于生成\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")  # 将张量移动到 CUDA 设备（GPU）\n",
        "\n",
        "# 使用模型生成输出\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs,  # 输入 ID\n",
        "    max_new_tokens=64,  # 最大生成的新标记数\n",
        "    use_cache=True,  # 使用缓存加速生成\n",
        "    temperature=1.5,  # 控制生成结果的随机性\n",
        "    min_p=0.1,  # 控制生成结果的多样性\n",
        ")\n",
        "\n",
        "# 将模型输出解码为文本\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "PHgyqsh8MVEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "您也可以使用 TextStreamer 进行连续推理 - 这样您就可以逐个标记地看到生成结果，而不是一直等待！"
      ],
      "metadata": {
        "id": "ItMDR8f8MbwQ"
      }
    },
    {
      "source": [
        "FastLanguageModel.for_inference(model) # 启用原生 2 倍速推理\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},  # 用户消息：继续斐波那契数列\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,  # 对消息进行分词\n",
        "    add_generation_prompt = True,  # 必须添加用于生成\n",
        "    return_tensors = \"pt\",  # 返回 PyTorch 张量\n",
        ").to(\"cuda\")  # 将张量移动到 CUDA 设备（GPU）\n",
        "\n",
        "from transformers import TextStreamer  # 导入 TextStreamer 用于连续推理\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)  # 创建 TextStreamer 实例\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,  # 使用模型生成输出\n",
        "                   use_cache = True, temperature = 1.5, min_p = 0.1)  # 设置生成参数"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7HsT7dlJMlKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 保存和加载微调模型\n",
        "要将最终模型保存为 LoRA 适配器，可以使用 Huggingface 的 push_to_hub 进行在线保存，或使用 save_pretrained 进行本地保存。\n",
        "\n",
        "[注意] 这只会保存 LoRA 适配器，而不是完整的模型。要保存为 16 位或 GGUF 格式，请向下滚动！"
      ],
      "metadata": {
        "id": "HEfn0Vx4MtS9"
      }
    },
    {
      "source": [
        "model.save_pretrained(\"lora_model\")  # 本地保存\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # 在线保存\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # 在线保存"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "hn9cK9seM0fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在，如果您想加载我们刚刚保存的 LoRA 适配器进行推理，请将 False 设置为 True："
      ],
      "metadata": {
        "id": "cj47yj8bM9GO"
      }
    },
    {
      "source": [
        "if False:  # 这段代码目前不会被执行，因为条件为 False\n",
        "    from unsloth import FastLanguageModel  # 从 unsloth 库导入 FastLanguageModel 类\n",
        "    # 从预训练模型加载模型和分词器\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"lora_model\",  # 你用于训练的模型名称，应替换为实际的模型名称\n",
        "        max_seq_length=max_seq_length,  # 最大序列长度，之前已定义\n",
        "        dtype=dtype,  # 数据类型，之前已定义\n",
        "        load_in_4bit=load_in_4bit,  # 是否使用 4 位量化加载，之前已定义\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)  # 启用原生 2 倍速推理\n",
        "\n",
        "# 定义消息列表，包含用户角色和内容\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Describe a tall tower in the capital of France.\"},  # 用户消息：描述法国首都的一座高塔\n",
        "]\n",
        "\n",
        "# 使用聊天模板处理消息，进行分词，添加生成提示，并转换为 PyTorch 张量\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,  # 对消息进行分词\n",
        "    add_generation_prompt=True,  # 必须添加用于生成\n",
        "    return_tensors=\"pt\",  # 返回 PyTorch 张量\n",
        ").to(\"cuda\")  # 将张量移动到 CUDA 设备（GPU）\n",
        "\n",
        "from transformers import TextStreamer  # 导入 TextStreamer 用于连续推理\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)  # 创建 TextStreamer 实例\n",
        "# 使用模型生成输出\n",
        "_ = model.generate(\n",
        "    input_ids=inputs,  # 输入 ID\n",
        "    streamer=text_streamer,  # 使用 TextStreamer 进行连续推理\n",
        "    max_new_tokens=128,  # 最大生成的新标记数\n",
        "    use_cache=True,  # 使用缓存加速生成\n",
        "    temperature=1.5,  # 控制生成结果的随机性\n",
        "    min_p=0.1,  # 控制生成结果的多样性\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "pP87O4QyNKO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "您也可以使用 Hugging Face 的 AutoModelForPeftCausalLM。仅当您未安装 unsloth 时才使用此方法。它可能会非常慢，因为它不支持 4bit 模型下载，而且 Unsloth 的 推理速度快 2 倍。\n",
        "\n"
      ],
      "metadata": {
        "id": "ugh5lYxRNTV7"
      }
    },
    {
      "source": [
        "if False:  # 这段代码目前不会被执行，因为条件为 False\n",
        "    # 我强烈不建议这样做 - 如果可能，请使用 Unsloth\n",
        "    from peft import AutoPeftModelForCausalLM  # 从 peft 库导入 AutoPeftModelForCausalLM 类\n",
        "    from transformers import AutoTokenizer  # 从 transformers 库导入 AutoTokenizer 类\n",
        "\n",
        "    # 从预训练模型加载模型，使用 LoRA 适配器\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"lora_model\",  # 你用于训练的模型名称，应替换为实际的模型名称\n",
        "        load_in_4bit=load_in_4bit,  # 是否使用 4 位量化加载，之前已定义\n",
        "    )\n",
        "    # 从预训练模型加载分词器\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")  # 你用于训练的模型名称，应替换为实际的模型"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Fms7SiMtNZAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 为 VLLM 保存为 float16 格式\n",
        "我们也支持直接保存为 float16 格式。选择 merged_16bit 用于 float16 或 merged_4bit 用于 int4。我们也允许使用 lora 适配器作为备用方案。使用 push_to_hub_merged 上传到您的 Hugging Face 帐户！您可以访问 [redacted link] 获取您的个人令牌。"
      ],
      "metadata": {
        "id": "JZWcVlQUNgz7"
      }
    },
    {
      "source": [
        "# 合并到 16 位\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# 合并到 4 位\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# 仅 LoRA 适配器\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "biKELkkLNncp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GGUF / llama.cpp 转换\n",
        "为了保存到 GGUF / llama.cpp，我们现在原生支持它！我们克隆了 llama.cpp，默认保存到 q8_0。我们允许所有方法，例如 q4_k_m。使用 save_pretrained_gguf 进行本地保存，使用 push_to_hub_gguf 上传到 HF。\n",
        "\n",
        "一些支持的量化方法（完整列表请参见我们的 [redacted link]）：\n",
        "\n",
        " - q8_0 - 快速转换。资源占用高，但通常可以接受。\n",
        " - q4_k_m - 推荐。对 attention.wv 和 feed_forward.w2 张量的一半使用 Q6_K，否则使用 Q4_K。\n",
        " - q5_k_m - 推荐。对 attention.wv 和 feed_forward.w2 张量的一半使用 Q6_K，否则使用 Q5_K。\n",
        "\n",
        "[新增] 要微调并自动导出到 Ollama，请尝试我们的 [redacted link]"
      ],
      "metadata": {
        "id": "Vj5u4wRbOogt"
      }
    },
    {
      "source": [
        "# 保存为 8 位 Q8_0 格式\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# 请记住访问 https://huggingface.co/settings/tokens 获取令牌！\n",
        "# 并将 hf 更改为您的用户名！\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# 保存为 16 位 GGUF 格式\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# 保存为 q4_k_m GGUF 格式\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# 保存为多种 GGUF 格式 - 如果您需要多种格式，这样会更快！\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # 将 hf 更改为您的用户名！\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # 在 https://huggingface.co/settings/tokens 获取令牌\n",
        "    )"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "A3gkLbO2O9qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在，在 llama.cpp 或基于 UI 的系统（如 Jan 或 Open WebUI）中使用 model-unsloth.gguf 文件或 model-unsloth-Q4_K_M.gguf 文件。您可以[redacted link]安装 Jan，[redacted link]安装 Open WebUI。\n",
        "大功告成！如果您对 Unsloth 有任何疑问，我们有一个 [redacted link] 频道！如果您发现任何错误或想了解最新的 LLM 信息，或者需要帮助、加入项目等，请随时加入我们的 Discord！\n",
        "\n",
        "其他一些链接：\n",
        "\n",
        "Llama 3.2 对话笔记本。[redacted link]\n",
        "将微调结果保存到 Ollama。[redacted link]\n",
        "Llama 3.2 视觉微调 - 放射学用例。[redacted link]\n",
        "在我们的[redacted link]中查看有关 DPO、ORPO、持续预训练、对话微调等内容的笔记本！\n",
        "  \n",
        "如果您需要帮助，请加入 Discord + ⭐️ 在 Github 上给我们点赞 ⭐️"
      ],
      "metadata": {
        "id": "ZbbATHScPHab"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}