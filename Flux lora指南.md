## 前言

写这篇文档的目的是，阐述我训练flux lora时得到的经验，根据这些经验，当你掌握了“如何训练flux lora”这种基础的时候，你可以利用这些经验**解决你的问题**，带来技术提升，实现不可思议的东西。

而没有这些经验，你只是学会了lora flux的皮毛，功力尚浅。

下面，我将逐步讲述这些。

## 一、什么情况下使用lora？

lora的核心意义是：

### 1、节流

说白了就是省钱，文件小、占用显存少、训练成本更低（服务器硬件配置要求低、训练时间短、数据集数量不需要太多）

### 2、专用

lora适合训练单一的概念，而不是多个概念。

例如，一条黄色的狗

例如，狗

例如，猫

而不是，猫、自行车、沙漠风景

多个概念不适合使用lora，你可以进行全量微调 full train

**所以，什么情况下使用lora？**

1、单一概念

2、你的数据集数量较少

3、你的预算非常低

如果你有以上其中一个，那么你应该使用lora。否则，你只是在找麻烦。

## 二、如何使用lora？

### 1、确保单个lora里面是单一概念，而不是多个概念

比如都是狗，或者都是猫。

如果非要多个概念，确保概念之间相近、类似，且概念的数量不多。

**如果你有多个概念，那么请使用多个lora，而不是单个lora。**

**如果你的概念之间相近，且概念数量在3个左右，可以尝试单个lora。**

### 2、如何在lora里面确定或者区分概念？

如果你有一个概念，那就是确定。

如果你有多个概念，那就是区分。

那么，如何实现？

非常简单，你只需要在数据集中，每个图片的描述中，添加唯一标识。

我的建议是**10个随机英文字母**。

**同一个概念的唯一标识是相同的，不同概念是不同的。**

添加之后，图片的描述看上去像这样：

```
This image is labeled as uehxbfklaq. A cat.
```

当你在生成图像的时候，可以这样来调用每个概念：

```
An image tagged with uehxbfklaq. A cat.
```

Flux的文本编码器使用了T5，它比sd模型要聪明的多，能理解这些。

### 3、概念之间的数据集数量占比！

如果你一个lora有多个概念，那么切记概念之间的样本数量占比非常重要。

如果总数为100张，概念1只有20张，概念2有80张，我们通常不会站在全局的视角观察到这个问题，但是需要观察到这个问题，否则你的概念1会在训练好的lora中消失的无影无踪。

如何解决这个问题？

分lora

分别使用不同的lora之后，你的20张数据集可以使用更少的步数step，节省成本，你的80张数据集可以使用之前你确定的步数，保持不变。

**即使是同一概念，但是不太一样，例如黄色的猫和黑色的猫作为同一个概念，但是黄猫黑猫样本占比不同，训练出来的lora会更倾向于一种猫，此时也建议分开lora。**

### 4、训练多少步数？

我的建议是 3～4 epochs之间，那么，假设批量大小为1，数据集有100张，梯度累积步数为1`gradient_accumulation_steps`，那么 4 epochs就是100x4=400。

也就是说，训练400 steps

如果你真的训练400 steps，那么应当注意检查点保存间隙、验证图像生成间隙，要改到400以内。

对于小数据集，如果可以进行更多的拟合，我一般是1500 steps或者2500 stpes，超过2500 steps就不太建议了。

小数据集是指100张左右的，对于500张左右的数据集适合2500～3500左右的steps，取决于你想更重拟合还是轻拟合。

这些估算建立在批量大小为1，梯度累积步数为1`gradient_accumulation_steps`情况下。

## 三、lora只是唤醒模型学习过的内容，如何训练模型没见过的东西？

### 1、lora只是唤醒模型学习过的内容

当你加载lora并推理的时候，对于生成图片所做的贡献，绝大多数都是由Flux模型学习过的内容完成的，lora只是一小部分。

举个例子，如果Flux模型从未学习过人像、人体，那么通过训练一个lora实现生成人像，是困难的。

所以，当你训练卡通人物、人像、图形的时候，lora并没有往模型中添加很多内容，lora只是基于模型已知的内容做微小调整。

由于Flux模型经过很多图像的训练，所以往往训练一个小lora就能实现你的目的。

但是，如果想让Flux模型生成它从未见过的内容呢？

这个例子非常不好举例，因为世界上的东西差不多，无论是图形还是物体，总有一些相似之处，但是，也确实有一些东西Flux不曾见过。

例如，中文文字、艺术字，Flux模型在生成它们的时候往往效果不好，似乎Flux没有训练过它们一样。

### 2、如何通过lora向Flux模型中添加更多内容，以实现生成Flux模型从未学习过的内容

- 数据集

样本数量应该在6000以上，这对lora来说已经是一个较高的样本数量。

- rank

这决定了lora训练的人工神经网络大小。

对于训练模型没见过的内容，建议的rank是128～256。

如果调整了rank，为了确保训练稳定，学习率lr需要调整。

建议的学习率如下：

```python
lora_ranks = [1, 16, 64, 128, 256]
learning_rates_by_rank = {
    1: "3e-4",
    16: "1e-4",
    64: "8e-5",
    128: "6e-5",
    256: "5.09e-5",
}
```

足够的rank能塞下更多信息，有助于让模型学会新内容。

如果rank太小，例如16，你会看到生成的内容模糊且扭曲，因为lora没有储存那么多信息，所以推理的时候只能用模糊来填充未知的新内容。

rank = 16，lora权重文件大小约为49MB

rank = 32，lora权重文件大小约为99MB

rank = 128，lora权重文件大小约为400MB

rank = 256，lora权重文件大小约为800MB

这会影响权重文件大小和训练&推理时占用的显存大小。

## 四、注意事项

### 1、数据增强

最小数据集大小为 train_batch_size * gradient_accumulation_steps，并且要大于 vae_batch_size。如果你的数据集太少，导致无法运行训练，可以将数据集复制一份。

切忌对数据集采用随机裁剪，这会加剧生成扭曲的内容。

### 2、验证图像

在训练lora的时候，你需要尽可能的**使用多张验证图像**，用于生成验证图像的提示词要涵盖你训练的数据集中的概念，你还可以添加一两个其他的提示词以验证模型是否遗忘其他内容。

这有助于帮助你监测训练情况，防止误判。

### 3、检查点

尽可能的多保留检查点，注意计算存储空间的占用情况，保留更多数量的检查点有助于你选择合适的检查点，但也会使用更存储空间。

### 4、检查点、验证图像、steps

假如检查点每隔 500 steps 保存一次，那么验证图像也需要每隔 500 steps 生成一次。

对于steps，假如通过与 4 epochs 计算得到 1234，那么你可以增加到1500或者降低到1000，这样与检查点是一致的。

### 5、数据集太少，导致模型总是学到背景而不是主体怎么办？

使用工具去除背景再训练

```bash
rembg p -m u2netp input_folder output_folder
```

## 五、例子

这里将通过几个简单的例子说明以上内容，方便理解。

### 1、人像与模特训练

我有20张模特的艺术照，如何训练一个生成该肖像的lora？

```python
rank = 32
lr = "1e-4"
steps = 500
检查点间隔 = 100
```

我想更拟合一点，比如包含人物的拍摄环境、服装、姿势

```python
rank = 32
lr = "1e-4"
steps = 1500
检查点间隔 = 500
```

### 2、两个模特的人像训练

我有60张艺术照，分别是两个模特的，每个模特30张，他们性别相同，年龄相仿，风格类似，拍摄地点类似，如何训练一个生成这俩肖像的lora？

```python
rank = 32
lr = "1e-4"
steps = 1500
检查点间隔 = 500
```

然后使用不同的唯一标识区分概念。

### 3、汽车、风景、野炊

我想训练一个lora，能够生成开着汽车自驾游欣赏风景的图片，我想发朋友圈装逼。我现在有100张实拍风景图，30张野炊图，1000张汽车图片。

建议使用三个lora，推理的时候加载三个lora并设置不同的影响占比。

100张实拍风景图

```python
rank = 32
lr = "1e-4"
steps = 1500
检查点间隔 = 500
```

30张野炊图

```python
rank = 32
lr = "1e-4"
steps = 500
检查点间隔 = 100
```

1000张汽车图片

```python
rank = 32
lr = "1e-4"
steps = 4000
检查点间隔 = 500
```
在推理的时候在提示词中调用多个唯一标识，精确控制图像。
